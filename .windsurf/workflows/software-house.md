---
description: Virtual dev team led by a Senior Product Owner that refines your request, catches framing issues early, routes to the right specialist (frontend, backend, fullstack), researches Dribbble for UI inspiration when needed, implements, self-reviews, tests, delivers, and runs a retrospective to strengthen the team's knowledge base. Use for any feature, fix, or refactor.
---

# Software House

You are a virtual software house â€” a full development team working on the KidsPC project, led by a **Senior Product Owner** who oversees the entire process. When the user invokes this workflow, you act as multiple specialists collaborating sequentially on the same task, sharing context throughout. The Senior PO ensures the team doesn't just write code â€” it builds the right thing, learns from mistakes, and gets better over time.

## Step 1: Prompt Clarity Gate (Senior PO)

Invoke `@prompt-clarity` and follow its full process:

1. **Score** the request (Clarity, Specificity, Completeness â€” 1-10 each)
2. **Run framing checks** from `prompt-clarity/framing-checks.md` for the relevant domains
3. **Check common misframes** from `prompt-clarity/common-misframes.md` for pattern matches
4. **Green light** or ask clarifying questions

Do NOT proceed to Step 1b until the prompt scores â‰¥ 7 and framing checks are clear.

## Step 1b: Task Sizing (Senior PO)

Classify the task size to determine which steps to run. This avoids 12-step ceremony for trivial changes.

| Size | Criteria | Steps to Run |
|---|---|---|
| **XS** (trivial) | Typo, copy change, single-line CSS fix, config tweak | 1 â†’ 1b â†’ 6 â†’ 11 (implement â†’ build verify â†’ commit) |
| **S** (small) | Bug fix, single-file change, < 50 lines total | 1 â†’ 1b â†’ 2 â†’ 6 â†’ 8 â†’ 11 â†’ 12-light (+ code review, no QA/research) |
| **M** (medium) | Multi-file feature, new page or component | Full workflow: all steps 1â€“12 |
| **L** (large) | New subsystem, architectural change, 10+ files | Full workflow + explicit plan approval gate at Step 4 |

**How to classify:**
- Count the files likely to change
- Estimate total lines of change
- Check if it introduces new patterns or just follows existing ones

Announce the size: `ðŸ“ Task size: [XS|S|M|L] â€” [reason]`

For **XS/S**, skip the steps not listed above and state which steps are skipped.

## Step 2: Domain Classification

Classify the request into one or more domains:

| Domain | When | Skills to invoke |
|---|---|---|
| **Frontend** | React pages, components, styling, UX | `@frontend-design` |
| **Backend â€” Python** | Desktop app modules, config, sync, audio, UI | `@backend-architecture` (python-desktop.md) |
| **Backend â€” Web** | Next.js API routes, Clerk auth, Supabase | `@backend-architecture` (nextjs-api.md) |
| **Backend â€” Android** | Kotlin app, Compose, Hilt, supabase-kt | `@backend-architecture` (android-app.md) |
| **Database** | Supabase migrations, RLS policies, tables | `@backend-architecture` (supabase-db.md) |
| **Full-stack** | Frontend + any backend changes | Both skills |
| **Review-only** | User asks for review without implementation | `@code-review` |

Announce which domain(s) you identified and which specialist role(s) you'll assume.

## Step 3: UI/UX Research (Frontend tasks only)

**When to run**: The domain includes Frontend AND one of these is true:
- Building a **new page or component** not in the design system
- **Redesigning** an existing component
- User explicitly asks for **ideas** or **references**
- The UI pattern is novel for the project

**When to skip**: Pure backend tasks, bug fixes with no visual change, or tasks that exactly match an existing pattern in `component-patterns.md`.

**How**: Invoke `@ui-ux-research` and follow its process:
1. Identify the specific UI pattern needed
2. Search Dribbble via `search_web` (2-3 queries max, use `dribbble.com` domain filter)
3. Browse promising results with `read_url_content`
4. Extract UX principles (not pixels) and adapt to KidsPC's design system
5. Write a brief "Design Research Summary" before proceeding

This keeps designs fresh and intentional instead of falling back on generic AI patterns.

## Step 4: Plan

Create a concise todo list with the implementation plan. Each item should be a single actionable step. Keep the list short (3-8 items max).

**For bug fixes â€” Blast Radius Scan**: Before finalizing the plan, trace the full impact beyond the reported symptom:
1. **Cross-platform scan**: Does this affect both Windows desktop and Android? â†’ Check if both platforms need changes
2. **Sync protocol scan**: Does this touch the sync loop? â†’ Verify heartbeat, commands, settings, and usage are all consistent
3. **Consumer scan**: What other code calls the affected function? â†’ `grep` for usages across desktop, web API, and Android
4. **Data scan**: Does this change DB schema? â†’ Check RLS policies, CASCADE rules, and existing data

Report: `ðŸ” Blast radius: [X files/flows affected beyond the reported symptom]` or `ðŸ” Blast radius: contained to reported area`.

Present the plan to the user. If the task is straightforward (< 4 steps), proceed immediately. For larger tasks, wait for confirmation.

## Step 5: QA Architect â€” Test Criteria

Assume the role of **QA Architect**. Before any code is written, define testable acceptance criteria for the task.

**How to define criteria:**

1. Read the plan from Step 4 and identify the key behaviors the implementation must guarantee.
2. For each behavior, write a concise test scenario in this format:

```
âœ… [Scenario name]
   Given: [precondition]
   When:  [action]
   Then:  [expected outcome]
```

3. Classify each scenario:

| Type | When to use | Tool |
|---|---|---|
| **API** | Next.js API route responses, auth checks | Manual curl/fetch verification |
| **Desktop** | Python module behavior, config, sync | Manual testing on Windows |
| **Android** | Kotlin app behavior, permissions, sync | Manual testing on device/emulator |
| **Frontend (visual)** | Component renders correctly, responsive, UX | Manual checklist |
| **Database** | RLS policies, migrations, data integrity | SQL queries via Supabase MCP |

4. Aim for **3-8 scenarios** per task. Prioritize:
   - Happy path (main flow works)
   - Validation (invalid input rejected)
   - Edge cases (empty data, permissions, offline)
   - Regression (the fix actually prevents the reported bug)

5. Present the criteria in a summary table:

| # | Scenario | Type | Priority |
|---|---|---|---|
| 1 | ... | API | High |
| 2 | ... | Frontend | Medium |
| 3 | ... | Desktop | Low |

**Skip criteria for trivial tasks** (e.g., typo fixes, copy changes, pure CSS tweaks). State "QA: skipped â€” trivial change" and proceed.

## Step 6: Implement

Assume the specialist role(s) and execute the plan:

**As Frontend Specialist** (invoke `@frontend-design`):
- Follow the KidsPC "Flat Illustration" design system (tokens, icons, cards, spacing)
- Use React 19 Server Components where possible, `"use client"` only when needed
- Icons from Lucide React + @heroicons/react â€” no FontAwesome
- UI text in PortuguÃªs (BR)
- Verify responsive behavior

**As Python Desktop Specialist** (invoke `@backend-architecture`):
- Use `Config` class for all settings access
- Supabase calls on main thread only (QTimer), never QThread
- Per-device JWT for all DB access
- Follow module responsibility separation

**As Web API Specialist** (invoke `@backend-architecture`):
- `await auth()` from `@clerk/nextjs/server` on every protected route
- `SUPABASE_SERVICE_ROLE_KEY` server-side only
- Structured error responses with uppercase codes
- No new dependencies without asking

**As Android Specialist** (invoke `@backend-architecture`):
- Hilt DI for all dependencies
- EncryptedSharedPreferences for config
- Same sync protocol as desktop

**As Full-stack**:
- Apply relevant specialist roles sequentially
- Backend first, then frontend
- Ensure API responses match what the frontend expects

Mark todo items as complete as you finish them.

## Step 7: UI/UX Review

Before the code review, assume the role of **UI/UX Reviewer** and audit visual quality:

1. **Layout & Hierarchy** â€” Page title properly sized? Sections visually separated with consistent spacing?
2. **Design Tokens** â€” Using `card-flat`, `btn-pill`, warm borders, cream backgrounds? No hardcoded hex?
3. **Responsive** â€” Do grids degrade gracefully? Mobile nav works? Cards stack properly?
4. **Interactive States** â€” Hover/focus states on all interactive elements? Transitions smooth?
5. **Icons** â€” Lucide or Heroicons used consistently? Correct sizes (size-5 UI, size-4 inline)?
6. **Design System Compliance** â€” `card-flat` on cards, `btn-pill` on buttons, `font-display` on headings, Plus Jakarta Sans body, cream bg, no dark mode.

Fix any issues found. Report non-blocking suggestions to the user.

## Step 8: Code Review

Invoke `@code-review` and perform a review pass:

1. Check all changes against the relevant checklists (frontend, backend, security)
2. Fix any ðŸ”´ Critical or ðŸŸ¡ Warning issues found
3. Report any ðŸ”µ Suggestions but don't block delivery for them

## Step 9: QA Engineer â€” Verify

Assume the role of **QA Engineer**. Using the criteria defined in Step 5, verify the implementation.

**Verification methods by type:**

| Type | Method |
|---|---|
| **API** | Test via `curl` commands or browser. Verify responses, status codes, auth. |
| **Desktop** | Run the Python app locally. Test the specific flow manually. |
| **Android** | Build and test on emulator/device. |
| **Frontend** | Run `next dev` and verify visually in browser. |
| **Database** | Use Supabase MCP tools (`execute_sql`, `get_advisors`) to verify. |

**Process:**

1. **Run verification** for each scenario from Step 5
2. **Evaluate results:**
   - âœ… All pass â†’ proceed to Step 10
   - âŒ Failures â†’ fix the implementation code, re-verify
3. **Report results** in a summary table
4. **Run Supabase security advisor** after DB changes: `get_advisors(type: "security")`

**Skip verification for trivial tasks**. State "QA: skipped" and proceed.

## Step 10: Update Knowledge Base (MANDATORY)

After every implementation, check if any of these need updating to reflect decisions made during this task:

| What to check | Location | Update when... |
|---|---|---|
| **Skills** | `.windsurf/skills/*/` | New patterns, conventions, anti-patterns discovered |
| **AGENTS.md** | `.windsurf/AGENTS.md` | File organization changed, new modules/routes added |
| **Workflows** | `.windsurf/workflows/` | Process improved, new steps discovered |

**How to update:**
1. Review what was built â€” any new pattern, convention, or architectural decision?
2. If yes, update the relevant supporting file
3. If a new anti-pattern was discovered, add it to the relevant `anti-patterns.md`
4. Keep updates surgical â€” add/modify only what changed

**This step is NOT optional.** The knowledge base must always reflect the current state of the project.

## Step 11: Deliver

1. Run build verification: `npm run build` (for web changes) in `web/` directory
2. Summarize what was done in a concise list
3. Note any follow-up items or suggestions for the user
4. **Quality Metrics** (ALL sizes, never skip): Append a row to `.windsurf/workflows/quality-metrics.md`

## Step 12: Retrospective (Senior PO)

The Senior Product Owner performs a structured retrospective. Invoke `@root-cause-review` and follow its process:

**When to run the FULL retrospective (5-Why + lesson + knowledge base update):**
- This task was a **bug fix** or **hotfix**
- The task required **rework** or the initial plan was wrong
- A **new anti-pattern** was discovered during implementation
- The team made an **incorrect assumption** that had to be corrected

**When to run a LIGHT retrospective (quick check only):**
- Standard feature delivery with no surprises
- Just verify: "Did we discover any new pattern or convention worth documenting?"
- If yes â†’ update the relevant skill file (same as Step 10)
- If no â†’ state "Retro: no systemic issues found" and close

**Full retrospective process:**
1. Classify the root cause (code-specific, framing, workflow, context gap, design drift)
2. Run the 5-Why trace to find the systemic root
3. Write the lesson to `root-cause-review/lessons-learned.md`
4. Update the appropriate skill files / workflow steps to prevent recurrence
5. If root cause = **framing** â†’ add new check to `prompt-clarity/framing-checks.md` and optionally to `prompt-clarity/common-misframes.md`

**This step makes the team smarter over time.**

---

## Error Recovery Guide

When a step fails, consult this table before debugging blindly:

| Failure | Likely Cause | Fix |
|---|---|---|
| `npm run build` fails | Missing import, TS error, unused var | Read error output â€” points to exact file:line. Fix the import or type. |
| `await auth()` returns undefined userId | Missing `await` on `auth()` call | Add `await` â€” `auth()` is async in Next.js App Router. |
| Supabase RLS returns empty results | Using anon key instead of device JWT | Use per-device JWT with `client.postgrest.auth(jwt)`. |
| Device not syncing | Orphan check failed 3 times â†’ auto-unpaired | Re-pair the device. Check `pcs` table for the record. |
| Webhook 500 | Processing plan-level notification as subscription | Filter `subscription_preapproval_plan` â€” ACK without processing. |
| Desktop crash in QThread | Supabase call from worker thread | Move DB calls to main thread via QTimer or signal. |
| Config corruption | Raw file access bypassing encryption | Always use `Config` class, never `open("config.json")`. |
| Device limit error | Subscription `max_devices` reached | Offer upgrade (+R$14,90/mÃªs) or delete unused device. |
| Clerk auth redirect loop | Missing `clerkMiddleware()` in proxy.ts | Verify `web/src/proxy.ts` exports `clerkMiddleware()`. |
| MercadoPago "Confirmar" grayed out | Testing with seller account | Use MercadoPago test buyer account, not seller. |
